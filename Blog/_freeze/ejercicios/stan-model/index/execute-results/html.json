{
  "hash": "a95e832ea5bf43f8b353a8b7e59d2194",
  "result": {
    "markdown": "---\ntitle: \"Modelación con Stan\"\n---\n\n\nlibrary(rstan)\nlibrary(tidyverse)\nrstan_options(auto_write = TRUE)\n\nLa modelación estadística tiene múltiples opciones actualmente. Señaladamente **R**, **Python**, **Julia**, **Matlab**, *Stata* y **Stan** que de acuerdo con su propia apreciación \"es una plataforma de última generación para el modelado estadístico y el cálculo estadístico de alto rendimiento en la que confían miles de usuarios\". **Stan** es básicamente un lenguaje para la programación de modelos probabilísticos que permite:\n\n+ Inferencia estadística bayesiana completa.\n+ Inferencia bayesiana aproximada con inferencia variacional.\n+ Estimación de máxima verosimilitud penalizada con optimización.\n\n**Stan** tiene su motor de cálculo propio, pero  interactúa muy bien con los lenguajes de análisis de datos más populares (*R*, *Python*, *MATLAB*, etc.) y en los sistemas operativos comunes (_Linux_, _Mac_, _Windows_). Puedes obtener más información sobre esta propuesta de modelación estadística en la [página de Stan](https://mc-stan.org/).\n\n\n## Preparación\n\nEl concepto que utiliza **Stan** parte de la especificación de las funciones de densidad que le interesan al usuario para enseguida ajustar los modelos a los datos. Un ejemplo trivial es el siguiente, en el que sólo nos proponemos estimar la media de una muestra de datos que asumiremos se distribuyen normalmente. Lo primero que hay que hacer es crear un archivo que especifique el modelo en los términos que **Stan** requiere. La especificación básica es la siguiente (se pueden agregar comentarios con un doble diagonal). Primero una definición del tipo de datos que requiere el modelo.\n\n``` stan\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\n```\n\nEn seguida se detallan los parámetros que definen el modelo.\n\n``` stan\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n```\n\nFinalmente, se especifica el modelo. El modelo completo se guarda como un archivo *stan* al que se llamará cuando se buusque realizar el ajuste.\n\n\n``` stan\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  y ~ normal(mu, sigma);\n}\n\n```\n\nPara ejemplificar el uso del modelo `stan` ya definido arriba, sólo nos resta preparar algunos datos de prueba, activar la biblioteca `rstan` y realizar el ajuste.\n\n\n\n::: {.cell}\n\n```{.r .bg-danger .cell-code}\nlibrary(rstan)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: StanHeaders\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nrstan version 2.32.5 (Stan version 2.32.2)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n```\n:::\n\n```{.r .bg-danger .cell-code}\nlibrary(tibble)\n\ndatos <-  list(N = 1000, \n               y = rnorm(1000, 10, 2))\n               \nfit <- stan(file = 'modelo.stan', data = datos)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 1:                0.031 seconds (Sampling)\nChain 1:                0.064 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.02 seconds (Warm-up)\nChain 2:                0.019 seconds (Sampling)\nChain 2:                0.039 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 3:                0.018 seconds (Sampling)\nChain 3:                0.041 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 4:                0.029 seconds (Sampling)\nChain 4:                0.059 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nAhora podemos ver los resultados del ajuste del modelo y explorar los resultados que arroja.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$summary\n              mean      se_mean         sd         2.5%         25%\nmu       10.051244 0.0009594207 0.05942588     9.933818    10.01085\nsigma     1.949976 0.0007515521 0.04389805     1.866640     1.91952\nlp__  -1166.394014 0.0208691960 0.94539161 -1168.931642 -1166.77844\n               50%          75%        97.5%    n_eff     Rhat\nmu       10.052132    10.091671    10.166510 3836.481 0.999823\nsigma     1.949208     1.978503     2.041439 3411.711 1.000019\nlp__  -1166.104877 -1165.718901 -1165.450903 2052.164 1.002159\n\n$c_summary\n, , chains = chain:1\n\n         stats\nparameter         mean         sd         2.5%         25%          50%\n    mu       10.051123 0.06103171     9.935396    10.00867    10.051193\n    sigma     1.950252 0.04525387     1.865501     1.91956     1.950025\n    lp__  -1166.451014 0.97738434 -1168.906305 -1166.84082 -1166.169905\n         stats\nparameter         75%        97.5%\n    mu       10.09437    10.165088\n    sigma     1.97890     2.045865\n    lp__  -1165.73694 -1165.449344\n\n, , chains = chain:2\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu       10.050543 0.05998280     9.924590    10.013517    10.054246\n    sigma     1.949912 0.04432913     1.864236     1.920175     1.946924\n    lp__  -1166.410710 0.96621193 -1168.977501 -1166.778930 -1166.117027\n         stats\nparameter          75%        97.5%\n    mu       10.091458    10.162434\n    sigma     1.980227     2.040094\n    lp__  -1165.726743 -1165.451634\n\n, , chains = chain:3\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu       10.052393 0.05780559     9.941550    10.013084    10.051954\n    sigma     1.950546 0.04452317     1.865282     1.918449     1.949414\n    lp__  -1166.382866 0.95613838 -1169.068699 -1166.767467 -1166.062803\n         stats\nparameter          75%        97.5%\n    mu       10.088801    10.173398\n    sigma     1.978926     2.044419\n    lp__  -1165.699221 -1165.450800\n\n, , chains = chain:4\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu       10.050915 0.05890770     9.935738    10.009850    10.051364\n    sigma     1.949191 0.04144421     1.873244     1.919731     1.949605\n    lp__  -1166.331467 0.87585802 -1168.674922 -1166.669638 -1166.058090\n         stats\nparameter          75%        97.5%\n    mu       10.091248    10.162434\n    sigma     1.976132     2.033293\n    lp__  -1165.702946 -1165.455457\n```\n:::\n\n```{.r .cell-code}\najuste <- as_tibble(rstan::extract(fit))\n\nhead(ajuste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n         mu     sigma      lp__\n  <dbl[1d]> <dbl[1d]> <dbl[1d]>\n1     10.1       1.96    -1166.\n2     10.1       1.97    -1167.\n3      9.96      1.93    -1167.\n4     10.0       1.87    -1167.\n5     10.0       1.94    -1165.\n6      9.99      1.94    -1166.\n```\n:::\n\n```{.r .cell-code}\nmean(ajuste$mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.05124\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}