{
  "hash": "9a951d8aeae1edb747c2438cbc89e54f",
  "result": {
    "markdown": "---\ntitle: \"Modelación con Stan\"\n---\n\n\nlibrary(rstan)\nlibrary(tidyverse)\nrstan_options(auto_write = TRUE)\n\n## Preparación\n\nHay que crear un archivo que especifique el modelo en los términos que **Stan** requiere. La especificación básica es la siguiente (se pueden agregar comentarios con un doble diagonal). Primero una definición del tipo de datos que requiere el modelo.\n\n```stan\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\n```\n\nEn seguida se detallan los parámetros que definen el modelo.\n\n```stan\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n```\n\nFinalmente, se especifica el modelo. El modelo completo se guarda como un archivo *stan* al que se llamará cuando se buusque realizar el ajuste.\n\n``` stan\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  y ~ normal(mu, sigma);\n}\n\n```\n\nPara ejemplificar el uso del modelo `stan` ya definido arriba, sólo nos resta preparar algunos datos de prueba, activar la biblioteca `rstan` y realizar el ajuste.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: StanHeaders\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nrstan version 2.32.5 (Stan version 2.32.2)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n```\n:::\n\n```{.r .cell-code}\nlibrary(tibble)\n\ndatos <-  list(N = 1000, \n               y = rnorm(1000, 10, 2))\n               \nfit <- stan(file = 'modelo.stan', data = datos)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrecompiling to avoid crashing R session\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 1:                0.025 seconds (Sampling)\nChain 1:                0.053 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 2:                0.029 seconds (Sampling)\nChain 2:                0.056 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 3:                0.033 seconds (Sampling)\nChain 3:                0.061 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 4:                0.032 seconds (Sampling)\nChain 4:                0.06 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nAhora podemos ver los resultados del ajuste del modelo y explorar los resultados que arroja.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$summary\n              mean      se_mean         sd         2.5%          25%\nmu       10.042666 0.0011334218 0.06382558     9.915417     9.999955\nsigma     1.979013 0.0007802688 0.04398799     1.895890     1.949253\nlp__  -1181.157859 0.0239558380 1.01765011 -1183.859863 -1181.522798\n               50%         75%        97.5%    n_eff     Rhat\nmu       10.043185    10.08572    10.168694 3171.074 1.000201\nsigma     1.978262     2.00872     2.068358 3178.188 1.001820\nlp__  -1180.833047 -1180.43098 -1180.173463 1804.572 1.001761\n\n$c_summary\n, , chains = chain:1\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu       10.040946 0.06365322     9.908818     9.997605    10.040414\n    sigma     1.979636 0.04417997     1.894838     1.949987     1.980158\n    lp__  -1181.160622 0.95315100 -1183.702130 -1181.515426 -1180.847679\n         stats\nparameter          75%        97.5%\n    mu       10.083883    10.161104\n    sigma     2.010114     2.069146\n    lp__  -1180.463386 -1180.174281\n\n, , chains = chain:2\n\n         stats\nparameter         mean         sd         2.5%          25%         50%\n    mu       10.042128 0.06068617     9.916300    10.001970    10.04526\n    sigma     1.977563 0.04124718     1.899988     1.950089     1.97537\n    lp__  -1181.048604 0.94891046 -1183.633858 -1181.390428 -1180.75372\n         stats\nparameter         75%        97.5%\n    mu       10.08183    10.152856\n    sigma     2.00598     2.056426\n    lp__  -1180.39164 -1180.173400\n\n, , chains = chain:3\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu       10.042737 0.06705434     9.913277     9.998900    10.041919\n    sigma     1.978566 0.04495607     1.894200     1.947543     1.978446\n    lp__  -1181.231511 1.14004820 -1184.216520 -1181.617289 -1180.862223\n         stats\nparameter          75%        97.5%\n    mu       10.085815    10.181772\n    sigma     2.007928     2.068984\n    lp__  -1180.417110 -1180.171164\n\n, , chains = chain:4\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu       10.044853 0.06378225     9.921218    10.000702    10.044765\n    sigma     1.980288 0.04546441     1.895560     1.949974     1.978187\n    lp__  -1181.190701 1.00918435 -1183.856591 -1181.565011 -1180.878186\n         stats\nparameter          75%        97.5%\n    mu       10.089972    10.171891\n    sigma     2.010247     2.069436\n    lp__  -1180.463887 -1180.175009\n```\n:::\n\n```{.r .cell-code}\najuste <- as_tibble(rstan::extract(fit))\nhead(ajuste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n         mu     sigma      lp__\n  <dbl[1d]> <dbl[1d]> <dbl[1d]>\n1     10.2       1.98    -1182.\n2      9.95      2.00    -1181.\n3     10.1       2.07    -1182.\n4     10.0       2.04    -1181.\n5     10.1       2.03    -1181.\n6     10.1       1.92    -1183.\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}